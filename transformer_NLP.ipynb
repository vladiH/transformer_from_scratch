{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transformer_NLP.ipynb","provenance":[],"collapsed_sections":["bihQwstPqytE","uVpwz_Rdq4Zu","_zoXmVEgu0rE","PcZjtoB9vaun","r0Gkpa5exWjb","_ST98-WlDXJ8","SxhJTZb1FTWA"],"toc_visible":true,"authorship_tag":"ABX9TyNywgm6h8zUS3RdXYiY848N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"GaI-pSB1pukx"},"source":["# **stage 1: import librarys**\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bihQwstPqytE"},"source":["## colab"]},{"cell_type":"code","metadata":{"id":"dFB59DI4o_7d"},"source":["#only for google colab\n","from google.colab import drive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O25jkcbNpMPA","executionInfo":{"status":"ok","timestamp":1607217882651,"user_tz":300,"elapsed":175628,"user":{"displayName":"google colab","photoUrl":"","userId":"11539701709700465395"}},"outputId":"c41a1bb7-4198-431c-bfbf-0f6ad16813a6"},"source":["drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hNyK7-rarQNy"},"source":["try:\n","  %tensorflow_version 2.x\n","except:\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uVpwz_Rdq4Zu"},"source":["## All devices"]},{"cell_type":"code","metadata":{"id":"AhGCQFGAqlAu"},"source":["import os\n","import numpy as np\n","import math\n","import re\n","import time\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","import tensorflow_datasets as tfds"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cFjCIVTzsRu_"},"source":["# **Stage 2: Preprocessing**\n","---"]},{"cell_type":"markdown","metadata":{"id":"_zoXmVEgu0rE"},"source":["## Extract dataset es-en from file targz\n","I download from this page https://www.statmt.org/europarl/"]},{"cell_type":"code","metadata":{"id":"S7WFph8mrw_g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607224076627,"user_tz":300,"elapsed":662,"user":{"displayName":"google colab","photoUrl":"","userId":"11539701709700465395"}},"outputId":"5250d01b-b235-42d9-c9a9-993a59cb0ec9"},"source":["%cd ./drive/MyDrive/Colab\\ Notebooks/investigacion/transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/investigacion/transformers\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PcZjtoB9vaun"},"source":["## Open data"]},{"cell_type":"code","metadata":{"id":"cFBOEznzt-uu"},"source":["with open('/content/drive/MyDrive/Colab Notebooks/investigacion/transformers/data/europarl-v7.es-en.en', \n","          mode='r',\n","          encoding='utf-8') as f:\n","  europarl_en = f.read()\n","\n","with open('/content/drive/MyDrive/Colab Notebooks/investigacion/transformers/data/europarl-v7.es-en.es', \n","          mode='r',\n","          encoding='utf-8') as f:\n","  europarl_es = f.read()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GDqR1L037V2r"},"source":["with open('/content/drive/MyDrive/Colab Notebooks/investigacion/transformers/data/europarl-v7.es-en.es', \n","          mode='r',\n","          encoding='utf-8') as f:\n","  europarl_es = f.read()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r0Gkpa5exWjb"},"source":["## Cleaning\n","Some basic cleaning only for test our implementation"]},{"cell_type":"code","metadata":{"id":"7fczJ46iyFfU"},"source":["corpus_en = europarl_en\n","corpus_es = europarl_es"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n1B3TONzwOYA"},"source":["corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\",'.###', corpus_en)\n","corpus_en = re.sub(r\"\\.###\",'', corpus_en)\n","corpus_en = re.sub(r\"  +\",' ', corpus_en)\n","corpus_en = corpus_en.split('\\n')\n","\n","corpus_es = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\",'.###', corpus_es)\n","corpus_es = re.sub(r\"\\.###\",'', corpus_es)\n","corpus_es = re.sub(r\"  +\",' ', corpus_es)\n","corpus_es = corpus_es.split('\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"06VcZXSszFxM"},"source":["## Tokenization\n","\n","the algorithm used is byte pair encoding: https://leimao.github.io/blog/Byte-Pair-Encoding/"]},{"cell_type":"code","metadata":{"id":"dYiM0_1azBRu"},"source":["# Build\n","tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus_en, target_vocab_size=2**13)\n","tokenizer_es = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus_es, target_vocab_size=2**13)\n","\n","tokenizer_en.save_to_file('./data/tokenizer_en')\n","tokenizer_es.save_to_file('./data/tokenizer_es')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fTMqdQSs2MQB"},"source":["#Load\n","tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.load_from_file('./data/tokenizer_en')\n","#ids = tokenizer_en.encode(\"hello world\")\n","#text = tokenizer_en.decode([1, 2, 3, 4])\n","\n","tokenizer_es = tfds.deprecated.text.SubwordTextEncoder.load_from_file('./data/tokenizer_es')\n","#ids = tokenizer_es.encode(\"hola mundo\")\n","#text = tokenizer_es.decode([1, 2, 3, 4])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7fiPvDh71CXd"},"source":["VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2\n","VOCAB_SIZE_ES = tokenizer_es.vocab_size + 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qQ8cYJUt9uVf","executionInfo":{"status":"ok","timestamp":1607106650152,"user_tz":300,"elapsed":97133,"user":{"displayName":"google colab","photoUrl":"","userId":"11539701709700465395"}},"outputId":"8633e63c-6314-424d-f79b-7a007e3c514a"},"source":["MAX_LENGTH = 0\n","for i in corpus_es:\n","  longitud = len(tokenizer_es.encode(i))\n","  if MAX_LENGTH < longitud:\n","    MAX_LENGTH = longitud\n","MAX_LENGTH"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1305"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"rGkHeBBV3xQa"},"source":["def encode(lang1, lang2):\n","  inputs = [VOCAB_SIZE_ES-2]+tokenizer_es.encode(lang1.numpy())+ [VOCAB_SIZE_ES-1]\n","  outputs = [VOCAB_SIZE_EN-2]+tokenizer_en.encode(lang2.numpy())+ [VOCAB_SIZE_EN-1]\n","  return inputs, outputs\n","\n","def tf_encode(es, en):\n","  result_es, result_en = tf.py_function(encode, [es, en], [tf.int64, tf.int64])\n","  result_es.set_shape([None])\n","  result_en.set_shape([None])\n","  return result_es, result_en"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_ST98-WlDXJ8"},"source":["## Remove too long sentences"]},{"cell_type":"code","metadata":{"id":"zPVs72PmDfvx"},"source":["MAX_LENGTH = 160\n","\n","def filter_max_length(x, y, max_length=MAX_LENGTH):\n","  return tf.logical_and(tf.size(x) <= max_length,\n","                        tf.size(y) <= max_length)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SxhJTZb1FTWA"},"source":["## Inputs and output for model"]},{"cell_type":"code","metadata":{"id":"wybJ5pNLGXiv"},"source":["BATCH_SIZE = 64\n","BUFFER_SIZE = len(corpus_es) # space where the shuffle is done 20000\n","dataset = tf.data.Dataset.from_tensor_slices((corpus_es, corpus_en))\n","\n","dataset = dataset.map(tf_encode)\n","dataset = dataset.filter(filter_max_length)\n","dataset = dataset.cache()\n","dataset = dataset.shuffle(BUFFER_SIZE)\n","dataset = dataset.padded_batch(BATCH_SIZE, padded_shapes= (MAX_LENGTH,MAX_LENGTH))\n","dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OlndiipUMTst"},"source":["# Stage 3: Model building\n","\n","I build the model taken from the next article https://arxiv.org/abs/1706.03762"]},{"cell_type":"markdown","metadata":{"id":"sdg8DZPrMYaB"},"source":["## Positional encoding\n","the positional encoding is defined as:\n","* $PE(pos, 2i)=sin(pos/1000^{2i/d_{model}})$\n","* $PE(pos, 2i+1)=cos(pos/1000^{2i/d_{model}})$\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"qbSgOFAjIGtd"},"source":["class PositionalEncoding(layers.Layer):\n","  def __init__(self):\n","    super(PositionalEncoding, self).__init__()\n","    \n","  def get_angles(self,pos, i, d_model):#pos:(seq_length,1), i:(1, d_model)\n","    angles = 1/(np.power(10000.,(2*(i//2))/d_model))\n","    return pos*angles #(seq_length, d_model)\n","  \n","  def call(self, inputs):\n","    seq_length = inputs.shape.as_list()[-2]\n","    d_model = inputs.shape.as_list()[-1]\n","    angles = self.get_angles(np.arange(seq_length)[:,np.newaxis],\n","                            np.arange(d_model)[np.newaxis,:],\n","                            d_model)\n","    angles[:,0::2] = np.sin(angles[:,0::2])\n","    angles[:,1::2] = np.cos(angles[:,1::2])\n","    pos_encoding =  angles[np.newaxis,...]\n","    return inputs + tf.cast(pos_encoding, tf.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S0mb1Lva01h2"},"source":["## Dot product attention computation\n","* $attention(Q,K,V) = softmax(QK^{T}/\\sqrt{d_{k}})V$\n"]},{"cell_type":"code","metadata":{"id":"UeKNZ5EypU6l"},"source":["def scaled_dot_product_attetion(queries, keys, values, mask):\n","  \"\"\"\n","    q: query shape == (..., seq_len_q, depth)\n","    k: key shape == (..., seq_len_k, depth)\n","    v: value shape == (..., seq_len_v, depth_v)\n","    mask: Float tensor with shape broadcastable \n","\n","    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n","    \"\"\"\n","  product = tf.matmul(queries, keys, transpose_b=True) # (..., seq_len_queries, seq_len_keys)\n","\n","  keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n","  scaled_dot_product = product/ tf.math.sqrt(keys_dim)\n","\n","  if mask is not None:\n","    scaled_dot_product += (mask*-1e9)\n","\n","  attention_weights = tf.nn.softmax(scaled_dot_product,axis=-1)\n","  output = tf.matmul(attention_weights, values) #(..., seq_len_q, depth_v)\n","\n","  #q(1, 3)\n","  #k(4, 3)\n","  #v(4, 2)\n","  #result = (1,3).t((4,3)) => (1,4)*(4,2) =>(1,2)=(q_len, v_dim)\n","  return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vmchBbLpC7si"},"source":["## Multihead attention"]},{"cell_type":"code","metadata":{"id":"i7pq3Jz3-wdY"},"source":["class MultiHeadAttention(layers.Layer):\n","  def __init__(self, num_heads):\n","    super(MultiHeadAttention, self).__init__()\n","    self.num_heads = num_heads\n","  \n","  def build(self, input_shape):\n","    self.d_model = input_shape[-1]\n","    assert self.d_model % self.num_heads == 0\n","\n","    self.depth = self.d_model//self.num_heads\n","\n","    self.wq = layers.Dense(self.d_model)\n","    self.wk = layers.Dense(self.d_model)\n","    self.wv = layers.Dense(self.d_model)\n","\n","    self.top_dense = layers.Dense(self.d_model)\n","\n","  def split_heads(self, inputs, batch_size):\n","    shape = (batch_size, -1, self.num_heads, self.depth)\n","    splited_inputs = tf.reshape(inputs, shape=shape)\n","    return tf.transpose(splited_inputs, perm=[0,2,1,3])#(batch_size,heads,seq_len,depth)\n","\n","  def call(self, queries, keys, values, mask):\n","    batch_size = tf.shape(queries)[0]\n","\n","    queries = self.wq(queries)# (batch_size, seq_len, d_model)\n","    keys = self.wk(keys)# (batch_size, seq_len, d_model)\n","    values = self.wv(values)# (batch_size, seq_len, d_model)\n","\n","\n","    queries = self.split_heads(queries, batch_size)# (batch_size, num_heads, seq_len_q, depth)\n","    keys = self.split_heads(keys, batch_size)# (batch_size, num_heads, seq_len_k, depth)\n","    values = self.split_heads(values, batch_size)# (batch_size, num_heads, seq_len_v, depth)\n","\n","    attention = scaled_dot_product_attetion(queries, keys, values, mask)\n","    attention = tf.transpose(attention, perm=[0,2,1,3])  \n","\n","    concat_attention = tf.reshape(attention, shape=(batch_size,-1, self.d_model))\n","    outputs = self.top_dense(concat_attention)\n","    return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZRvEZuBNaJwo"},"source":["## Encoder"]},{"cell_type":"code","metadata":{"id":"aL1_wkfsaLxT"},"source":["class EncoderLayer(layers.Layer):\n","  def __init__(self,\n","               ffn_units, num_heads, dropout):\n","    super(EncoderLayer,self).__init__()\n","    self.ffn_units = ffn_units\n","    self.num_heads = num_heads\n","    self.dropout = dropout\n","  \n","  def build(self, inputs_shape):\n","    self.d_model = inputs_shape[-1]\n","    self.multi_head_attention = MultiHeadAttention(self.num_heads)\n","    self.dropout_1 = layers.Dropout(rate = self.dropout)\n","    self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n","\n","    self.dense_1 = layers.Dense(units = self.ffn_units, activation=\"relu\")\n","    self.dense_2 = layers.Dense(units = self.d_model)\n","    self.dropout_2 = layers.Dropout(rate = self.dropout)\n","    self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n","\n","  def call(self, inputs, mask, training):\n","    attention = self.multi_head_attention(inputs,inputs,inputs, mask)\n","    attention = self.dropout_1(attention, training = training)\n","    attention = self.norm_1(attention+inputs)\n","\n","    outputs = self.dense_1(attention)\n","    outputs = self.dense_2(outputs)\n","    outputs = self.dropout_2(outputs, training = training)\n","    outputs = self.norm_2(outputs)\n","    return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TY231npNg4ND"},"source":["class Encoder(layers.Layer):\n","  def __init__(self, nb_layers, \n","               ffn_units, \n","               num_heads, \n","               dropout, \n","               vocab_size,\n","               d_model,\n","               name=\"Encoder\"):\n","    super(Encoder, self).__init__(name=name)\n","    self.nb_layers = nb_layers\n","    self.d_model = d_model\n","    self.embedding = layers.Embedding(vocab_size, d_model)\n","    self.pos_encoding = PositionalEncoding()\n","    self.dropout = layers.Dropout(rate=dropout)\n","    self.enc_layers = [EncoderLayer(ffn_units,\n","                                    num_heads, \n","                                    dropout) for _ in range(nb_layers)]\n","\n","  def call(self, inputs, mask, training):\n","    outputs = self.embedding(inputs)\n","    outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","    #outputs = self.pos_encoding(outputs)\n","    outputs = self.dropout(outputs, training=training)\n","\n","    for i in range(self.nb_layers):\n","      outputs = self.enc_layers[i](outputs, mask, training)\n","\n","    return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sNkXyO5olW5w"},"source":["## Decoder"]},{"cell_type":"code","metadata":{"id":"S9FiTMYmlYeo"},"source":["class DecoderLayer(layers.Layer):\n","  def __init__(self, ffn_units, num_head, dropout):\n","    super(DecoderLayer, self).__init__()\n","    self.ffn_units = ffn_units\n","    self.num_head = num_head\n","    self.dropout = dropout\n","  \n","  def build(self, input_shape):\n","    self.d_model = input_shape[-1]\n","\n","    self.multi_head_attention_1 = MultiHeadAttention(self.num_head)\n","    self.dropout_1 = layers.Dropout(rate=self.dropout)\n","    self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n","\n","    self.multi_head_attention_2 = MultiHeadAttention(self.num_head)\n","    self.dropout_2 = layers.Dropout(rate=self.dropout)\n","    self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n","\n","    self.dense_1 = layers.Dense(units = self.ffn_units, activation=\"relu\")\n","    self.dense_2 = layers.Dense(units = self.d_model)\n","    self.dropout_3 = layers.Dropout(rate = self.dropout)\n","    self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n","\n","  def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n","    attention = self.multi_head_attention_1(inputs,\n","                                            inputs,\n","                                            inputs,\n","                                            mask_1)\n","    attention = self.dropout_1(attention, training)\n","    attention = self.norm_1(attention + inputs)\n","\n","    attention_2 = self.multi_head_attention_2(attention,\n","                                            enc_outputs,\n","                                            enc_outputs,\n","                                            mask_2)\n","    attention_2 = self.dropout_2(attention_2, training)\n","    attention_2 = self.norm_2(attention_2 + inputs)\n","\n","    outputs = self.dense_1(attention_2)\n","    outputs = self.dense_2(outputs)\n","    outputs = self.dropout_3(outputs, training = training)\n","    outputs = self.norm_3(outputs+attention_2)\n","\n","    return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zqSCqlW5pYvR"},"source":["class Decoder(layers.Layer):\n","  def __init__(self,\n","              nb_layers,\n","              ffn_units,\n","              num_heads,\n","              dropout,\n","              vocab_size,\n","              d_model,\n","              name=\"decoder\"):\n","    super(Decoder,self).__init__(name=name)\n","    self.d_model = d_model\n","    self.nb_layers = nb_layers\n","    self.embedding = layers.Embedding(vocab_size, d_model)\n","    self.pos_encoding = PositionalEncoding()\n","    self.dropout = layers.Dropout(rate=dropout)\n","    self.dec_layers = [DecoderLayer(ffn_units, \n","                                    num_heads, \n","                                    dropout) for _ in range(nb_layers)]\n","\n","  def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n","    outputs = self.embedding(inputs)\n","    outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","    outputs = self.pos_encoding(outputs)\n","    outputs = self.dropout(outputs, training=training)\n","\n","    for i in range(self.nb_layers):\n","      outputs = self.dec_layers[i](outputs, enc_outputs, mask_1, mask_2, training)\n","\n","    return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ONSiAJ7BsU4Z"},"source":["## Transformer"]},{"cell_type":"code","metadata":{"id":"Tsjl4I74sXXz"},"source":["class Transformer(tf.keras.Model):\n","  def __init__(self, vocab_size_enc,\n","               vocab_size_dec,\n","               d_model,\n","               nb_layers,\n","               ffn_units,\n","               num_heads,\n","               dropout,\n","               name=\"transformer\"):\n","    super(Transformer, self).__init__(name = name)\n","    self.encoder = Encoder(nb_layers,\n","                          ffn_units,\n","                          num_heads,\n","                          dropout,\n","                          vocab_size_enc,\n","                          d_model)\n","    self.decoder = Decoder(nb_layers,\n","                          ffn_units,\n","                          num_heads,\n","                          dropout,\n","                          vocab_size_dec,\n","                          d_model)\n","    \n","    self.last_dense = layers.Dense(units = vocab_size_dec)\n","\n","  def create_padding_mask(self, seq):\n","    mask = tf.cast(tf.math.equal(seq,0), tf.float32)\n","    return mask[:, tf.newaxis, tf.newaxis, :] \n","\n","  def create_look_ahead_mask(self,seq):\n","    seq_len = tf.shape(seq)[1]\n","    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)),-1,0)\n","    return look_ahead_mask\n","\n","  def call(self, enc_inputs, dec_inputs, training):\n","    enc_mask = self.create_padding_mask(enc_inputs)\n","    dec_mask_1 = tf.maximum(\n","        self.create_padding_mask(dec_inputs),\n","        self.create_look_ahead_mask(dec_inputs)\n","    )\n","    dec_mask_2 = self.create_padding_mask(enc_inputs)\n","    enc_ouputs = self.encoder(enc_inputs, enc_mask, training)\n","    dec_outputs = self.decoder(dec_inputs,\n","                               enc_ouputs, \n","                               dec_mask_1,\n","                               dec_mask_2,\n","                               training)\n","    outputs = self.last_dense(dec_outputs)\n","    return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bmYovFwXjUYx"},"source":["## Training"]},{"cell_type":"markdown","metadata":{"id":"2HuwMzhPjZWw"},"source":["### Hyper-parameters"]},{"cell_type":"code","metadata":{"id":"i2jU366tjWrS"},"source":["tf.keras.backend.clear_session()\n","D_MODEL = 256 #512\n","NB_LAYERS = 4 #6\n","FFN_UNITS = 2048 #2048\n","NB_HEADS = 8 #8\n","DROPOUT = 0.1 #0.1\n","\n","transformer = Transformer(vocab_size_enc = VOCAB_SIZE_ES,\n","               #vocab_size_dec = VOCAB_SIZE_EN,\n","               vocab_size_dec = VOCAB_SIZE_ES,\n","               d_model = D_MODEL,\n","               nb_layers = NB_LAYERS,\n","               ffn_units = FFN_UNITS,\n","               num_heads = NB_HEADS,\n","               dropout = DROPOUT)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fqhn9SIfj-Kj"},"source":["### Loss and accuracy function"]},{"cell_type":"code","metadata":{"id":"PpwCf2kMj71K"},"source":["loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","def loss_function(target, pred):\n","  mask = tf.math.logical_not(tf.math.equal(target, 0))\n","  loss_ = loss_object(target, pred)\n","\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","\n","  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QUspZ3u2trpQ"},"source":["def accuracy_function(target, pred):\n","  accuracy = tf.equal(target, tf.argmax(pred, axis=2))\n","\n","  mask = tf.math.logical_not(tf.math.equal(target, 0))\n","  accuracy = tf.math.logical_and(mask, accuracy)\n","\n","  accuracy = tf.cast(accuracy, dtype=tf.float32)\n","  mask = tf.cast(mask, dtype=tf.float32)\n","  return tf.reduce_sum(accuracy)/tf.reduce_sum(mask)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xazi76j8lZb2"},"source":["### Custom Schedule\n","$lrate = d_{model}^{-0.5}*min({steps}^{-0.5},\\mbox{step_num}*\\mbox{warmup_steps}^{-0.5})$"]},{"cell_type":"code","metadata":{"id":"4XLn5rMPldi-"},"source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","  def __init__(self, d_model, warmup_steps=4000):\n","    super(CustomSchedule, self).__init__()\n","\n","    self.d_model = d_model\n","    self.d_model = tf.cast(self.d_model, tf.float32)\n","\n","    self.warmup_steps = warmup_steps\n","\n","  def __call__(self, step):\n","    arg1 = tf.math.rsqrt(step)\n","    arg2 = step * (self.warmup_steps ** -1.5)\n","\n","    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n","\n","learning_rate = CustomSchedule(D_MODEL)\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n","                                     epsilon=1e-9)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3POI1c4XqwR9"},"source":["### Metrics"]},{"cell_type":"code","metadata":{"id":"Vqy4biT_nn7P"},"source":["train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n","val_loss = tf.keras.metrics.Mean(name='val_loss')\n","val_accuracy = tf.keras.metrics.Mean(name='val_accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TCJwrKhwrT2q"},"source":["### Checkpoint setting"]},{"cell_type":"code","metadata":{"id":"T44-3sBurThJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607224198814,"user_tz":300,"elapsed":1671,"user":{"displayName":"google colab","photoUrl":"","userId":"11539701709700465395"}},"outputId":"2343f868-cd5a-427f-e574-75522d7c2055"},"source":["checkpoint_path = './ckpt'\n","ckpt = tf.train.Checkpoint(transformer = transformer, \n","                           optimizer=optimizer)\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","if(ckpt_manager.latest_checkpoint):\n","  ckpt.restore(ckpt_manager.latest_checkpoint)\n","  print('latest_checkpoint restored')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["latest_checkpoint restored\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Li1710GVxJr7"},"source":["### Training"]},{"cell_type":"code","metadata":{"id":"Mw17P3-4xIa7"},"source":["train_step_signature = [\n","    tf.TensorSpec(shape=(None, MAX_LENGTH), dtype=tf.int64),\n","    tf.TensorSpec(shape=(None, MAX_LENGTH), dtype=tf.int64),\n","]\n","\n","@tf.function(input_signature=train_step_signature)\n","def train_step(inp, tar):\n","  tar_inp = tar[:, :-1]\n","  tar_real = tar[:, 1:]\n","  with tf.GradientTape() as tape:\n","    predictions = transformer(inp, tar_inp, \n","                                 True)\n","    loss = loss_function(tar_real, predictions)\n","\n","  gradients = tape.gradient(loss, transformer.trainable_weights)    \n","  optimizer.apply_gradients(zip(gradients, transformer.trainable_weights))\n","\n","  train_loss.update_state(loss)\n","  train_accuracy.update_state(accuracy_function(tar_real, predictions))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Um_LkjRiyS05","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e9dd9fd3-4e4a-4063-8665-d2458c8d043a"},"source":["#I stop the model only in two epochs due to lack of time\n","EPOCHS=2\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  train_loss.reset_states()\n","  train_accuracy.reset_states()\n","\n","  # inp -> espanish, tar -> english\n","  for (batch, (inp, tar)) in enumerate(dataset):\n","    train_step(inp, tar)\n","\n","    if batch % 50 == 0:\n","      print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n","                                                train_loss.result(), \n","                                                train_accuracy.result()))\n","  #if (epoch + 1) % 5 == 0:\n","  ckpt_save_path = ckpt_manager.save()\n","  print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                         ckpt_save_path))\n","\n","  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n","                                                train_loss.result(), \n","                                                train_accuracy.result()))\n","\n","  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1 Loss 9.0188 Accuracy 0.0000\n","Epoch 1 Loss 8.8804 Accuracy 0.0289\n","Epoch 1 Loss 8.7290 Accuracy 0.0401\n","Epoch 1 Loss 8.5899 Accuracy 0.0488\n","Epoch 1 Loss 8.4231 Accuracy 0.0595\n","Epoch 1 Loss 8.2353 Accuracy 0.0693\n","Epoch 1 Loss 8.0370 Accuracy 0.0791\n","Epoch 1 Loss 7.8448 Accuracy 0.0886\n","Epoch 1 Loss 7.6659 Accuracy 0.0985\n","Epoch 1 Loss 7.5007 Accuracy 0.1083\n","Epoch 1 Loss 7.3454 Accuracy 0.1178\n","Epoch 1 Loss 7.1995 Accuracy 0.1267\n","Epoch 1 Loss 7.0637 Accuracy 0.1351\n","Epoch 1 Loss 6.9383 Accuracy 0.1428\n","Epoch 1 Loss 6.8204 Accuracy 0.1501\n","Epoch 1 Loss 6.7109 Accuracy 0.1570\n","Epoch 1 Loss 6.6101 Accuracy 0.1632\n","Epoch 1 Loss 6.5166 Accuracy 0.1691\n","Epoch 1 Loss 6.4287 Accuracy 0.1748\n","Epoch 1 Loss 6.3474 Accuracy 0.1800\n","Epoch 1 Loss 6.2691 Accuracy 0.1853\n","Epoch 1 Loss 6.1963 Accuracy 0.1901\n","Epoch 1 Loss 6.1263 Accuracy 0.1950\n","Epoch 1 Loss 6.0608 Accuracy 0.1995\n","Epoch 1 Loss 5.9978 Accuracy 0.2039\n","Epoch 1 Loss 5.9375 Accuracy 0.2082\n","Epoch 1 Loss 5.8808 Accuracy 0.2123\n","Epoch 1 Loss 5.8257 Accuracy 0.2163\n","Epoch 1 Loss 5.7722 Accuracy 0.2203\n","Epoch 1 Loss 5.7211 Accuracy 0.2242\n","Epoch 1 Loss 5.6704 Accuracy 0.2282\n","Epoch 1 Loss 5.6216 Accuracy 0.2320\n","Epoch 1 Loss 5.5736 Accuracy 0.2359\n","Epoch 1 Loss 5.5263 Accuracy 0.2398\n","Epoch 1 Loss 5.4803 Accuracy 0.2438\n","Epoch 1 Loss 5.4354 Accuracy 0.2476\n","Epoch 1 Loss 5.3907 Accuracy 0.2516\n","Epoch 1 Loss 5.3466 Accuracy 0.2556\n","Epoch 1 Loss 5.3035 Accuracy 0.2596\n","Epoch 1 Loss 5.2612 Accuracy 0.2635\n","Epoch 1 Loss 5.2186 Accuracy 0.2675\n","Epoch 1 Loss 5.1774 Accuracy 0.2713\n","Epoch 1 Loss 5.1371 Accuracy 0.2751\n","Epoch 1 Loss 5.0970 Accuracy 0.2790\n","Epoch 1 Loss 5.0572 Accuracy 0.2829\n","Epoch 1 Loss 5.0182 Accuracy 0.2867\n","Epoch 1 Loss 4.9799 Accuracy 0.2905\n","Epoch 1 Loss 4.9416 Accuracy 0.2943\n","Epoch 1 Loss 4.9037 Accuracy 0.2982\n","Epoch 1 Loss 4.8666 Accuracy 0.3019\n","Epoch 1 Loss 4.8297 Accuracy 0.3056\n","Epoch 1 Loss 4.7938 Accuracy 0.3092\n","Epoch 1 Loss 4.7581 Accuracy 0.3128\n","Epoch 1 Loss 4.7229 Accuracy 0.3164\n","Epoch 1 Loss 4.6888 Accuracy 0.3198\n","Epoch 1 Loss 4.6550 Accuracy 0.3233\n","Epoch 1 Loss 4.6213 Accuracy 0.3267\n","Epoch 1 Loss 4.5890 Accuracy 0.3300\n","Epoch 1 Loss 4.5565 Accuracy 0.3334\n","Epoch 1 Loss 4.5243 Accuracy 0.3367\n","Epoch 1 Loss 4.4928 Accuracy 0.3400\n","Epoch 1 Loss 4.4614 Accuracy 0.3432\n","Epoch 1 Loss 4.4309 Accuracy 0.3464\n","Epoch 1 Loss 4.4009 Accuracy 0.3495\n","Epoch 1 Loss 4.3712 Accuracy 0.3526\n","Epoch 1 Loss 4.3422 Accuracy 0.3556\n","Epoch 1 Loss 4.3136 Accuracy 0.3585\n","Epoch 1 Loss 4.2859 Accuracy 0.3614\n","Epoch 1 Loss 4.2584 Accuracy 0.3643\n","Epoch 1 Loss 4.2316 Accuracy 0.3671\n","Epoch 1 Loss 4.2049 Accuracy 0.3699\n","Epoch 1 Loss 4.1785 Accuracy 0.3726\n","Epoch 1 Loss 4.1524 Accuracy 0.3754\n","Epoch 1 Loss 4.1270 Accuracy 0.3780\n","Epoch 1 Loss 4.1023 Accuracy 0.3806\n","Epoch 1 Loss 4.0777 Accuracy 0.3832\n","Epoch 1 Loss 4.0538 Accuracy 0.3857\n","Epoch 1 Loss 4.0300 Accuracy 0.3882\n","Epoch 1 Loss 4.0071 Accuracy 0.3905\n","Epoch 1 Loss 3.9841 Accuracy 0.3929\n","Epoch 1 Loss 3.9620 Accuracy 0.3952\n","Epoch 1 Loss 3.9400 Accuracy 0.3975\n","Epoch 1 Loss 3.9186 Accuracy 0.3997\n","Epoch 1 Loss 3.8972 Accuracy 0.4019\n","Epoch 1 Loss 3.8759 Accuracy 0.4042\n","Epoch 1 Loss 3.8549 Accuracy 0.4063\n","Epoch 1 Loss 3.8339 Accuracy 0.4086\n","Epoch 1 Loss 3.8135 Accuracy 0.4107\n","Epoch 1 Loss 3.7931 Accuracy 0.4128\n","Epoch 1 Loss 3.7732 Accuracy 0.4149\n","Epoch 1 Loss 3.7532 Accuracy 0.4171\n","Epoch 1 Loss 3.7336 Accuracy 0.4191\n","Epoch 1 Loss 3.7141 Accuracy 0.4212\n","Epoch 1 Loss 3.6951 Accuracy 0.4232\n","Epoch 1 Loss 3.6763 Accuracy 0.4252\n","Epoch 1 Loss 3.6582 Accuracy 0.4271\n","Epoch 1 Loss 3.6397 Accuracy 0.4291\n","Epoch 1 Loss 3.6220 Accuracy 0.4309\n","Epoch 1 Loss 3.6040 Accuracy 0.4329\n","Epoch 1 Loss 3.5867 Accuracy 0.4347\n","Epoch 1 Loss 3.5694 Accuracy 0.4366\n","Epoch 1 Loss 3.5521 Accuracy 0.4384\n","Epoch 1 Loss 3.5353 Accuracy 0.4402\n","Epoch 1 Loss 3.5186 Accuracy 0.4420\n","Epoch 1 Loss 3.5019 Accuracy 0.4438\n","Epoch 1 Loss 3.4860 Accuracy 0.4455\n","Epoch 1 Loss 3.4699 Accuracy 0.4472\n","Epoch 1 Loss 3.4543 Accuracy 0.4489\n","Epoch 1 Loss 3.4388 Accuracy 0.4505\n","Epoch 1 Loss 3.4235 Accuracy 0.4522\n","Epoch 1 Loss 3.4083 Accuracy 0.4538\n","Epoch 1 Loss 3.3933 Accuracy 0.4554\n","Epoch 1 Loss 3.3785 Accuracy 0.4570\n","Epoch 1 Loss 3.3634 Accuracy 0.4586\n","Epoch 1 Loss 3.3488 Accuracy 0.4602\n","Epoch 1 Loss 3.3346 Accuracy 0.4617\n","Epoch 1 Loss 3.3206 Accuracy 0.4633\n","Epoch 1 Loss 3.3066 Accuracy 0.4647\n","Epoch 1 Loss 3.2929 Accuracy 0.4662\n","Epoch 1 Loss 3.2791 Accuracy 0.4677\n","Epoch 1 Loss 3.2657 Accuracy 0.4691\n","Epoch 1 Loss 3.2524 Accuracy 0.4706\n","Epoch 1 Loss 3.2390 Accuracy 0.4721\n","Epoch 1 Loss 3.2261 Accuracy 0.4735\n","Epoch 1 Loss 3.2136 Accuracy 0.4749\n","Epoch 1 Loss 3.2009 Accuracy 0.4762\n","Epoch 1 Loss 3.1884 Accuracy 0.4776\n","Epoch 1 Loss 3.1759 Accuracy 0.4789\n","Epoch 1 Loss 3.1636 Accuracy 0.4803\n","Epoch 1 Loss 3.1517 Accuracy 0.4815\n","Epoch 1 Loss 3.1395 Accuracy 0.4829\n","Epoch 1 Loss 3.1276 Accuracy 0.4842\n","Epoch 1 Loss 3.1158 Accuracy 0.4855\n","Epoch 1 Loss 3.1044 Accuracy 0.4867\n","Epoch 1 Loss 3.0931 Accuracy 0.4880\n","Epoch 1 Loss 3.0817 Accuracy 0.4892\n","Epoch 1 Loss 3.0706 Accuracy 0.4904\n","Epoch 1 Loss 3.0596 Accuracy 0.4916\n","Epoch 1 Loss 3.0485 Accuracy 0.4929\n","Epoch 1 Loss 3.0377 Accuracy 0.4940\n","Epoch 1 Loss 3.0269 Accuracy 0.4952\n","Epoch 1 Loss 3.0163 Accuracy 0.4964\n","Epoch 1 Loss 3.0059 Accuracy 0.4976\n","Epoch 1 Loss 2.9955 Accuracy 0.4987\n","Epoch 1 Loss 2.9854 Accuracy 0.4998\n","Epoch 1 Loss 2.9753 Accuracy 0.5010\n","Epoch 1 Loss 2.9653 Accuracy 0.5020\n","Epoch 1 Loss 2.9555 Accuracy 0.5031\n","Epoch 1 Loss 2.9457 Accuracy 0.5042\n","Epoch 1 Loss 2.9360 Accuracy 0.5053\n","Epoch 1 Loss 2.9261 Accuracy 0.5064\n","Epoch 1 Loss 2.9166 Accuracy 0.5074\n","Epoch 1 Loss 2.9074 Accuracy 0.5085\n","Epoch 1 Loss 2.8980 Accuracy 0.5095\n","Epoch 1 Loss 2.8888 Accuracy 0.5105\n","Epoch 1 Loss 2.8795 Accuracy 0.5116\n","Epoch 1 Loss 2.8705 Accuracy 0.5126\n","Epoch 1 Loss 2.8613 Accuracy 0.5136\n","Epoch 1 Loss 2.8524 Accuracy 0.5146\n","Epoch 1 Loss 2.8438 Accuracy 0.5156\n","Epoch 1 Loss 2.8349 Accuracy 0.5166\n","Epoch 1 Loss 2.8263 Accuracy 0.5175\n","Epoch 1 Loss 2.8177 Accuracy 0.5185\n","Epoch 1 Loss 2.8092 Accuracy 0.5194\n","Epoch 1 Loss 2.8009 Accuracy 0.5204\n","Epoch 1 Loss 2.7925 Accuracy 0.5213\n","Epoch 1 Loss 2.7845 Accuracy 0.5222\n","Epoch 1 Loss 2.7763 Accuracy 0.5231\n","Epoch 1 Loss 2.7682 Accuracy 0.5240\n","Epoch 1 Loss 2.7601 Accuracy 0.5249\n","Epoch 1 Loss 2.7521 Accuracy 0.5259\n","Epoch 1 Loss 2.7443 Accuracy 0.5267\n","Epoch 1 Loss 2.7365 Accuracy 0.5276\n","Epoch 1 Loss 2.7287 Accuracy 0.5285\n","Epoch 1 Loss 2.7212 Accuracy 0.5293\n","Epoch 1 Loss 2.7137 Accuracy 0.5302\n","Epoch 1 Loss 2.7061 Accuracy 0.5310\n","Epoch 1 Loss 2.6987 Accuracy 0.5319\n","Epoch 1 Loss 2.6911 Accuracy 0.5327\n","Epoch 1 Loss 2.6839 Accuracy 0.5336\n","Epoch 1 Loss 2.6767 Accuracy 0.5344\n","Epoch 1 Loss 2.6695 Accuracy 0.5352\n","Epoch 1 Loss 2.6622 Accuracy 0.5360\n","Epoch 1 Loss 2.6552 Accuracy 0.5368\n","Epoch 1 Loss 2.6483 Accuracy 0.5376\n","Epoch 1 Loss 2.6413 Accuracy 0.5384\n","Epoch 1 Loss 2.6344 Accuracy 0.5392\n","Epoch 1 Loss 2.6276 Accuracy 0.5400\n","Epoch 1 Loss 2.6208 Accuracy 0.5408\n","Epoch 1 Loss 2.6141 Accuracy 0.5415\n","Epoch 1 Loss 2.6076 Accuracy 0.5422\n","Epoch 1 Loss 2.6012 Accuracy 0.5430\n","Epoch 1 Loss 2.5946 Accuracy 0.5437\n","Epoch 1 Loss 2.5881 Accuracy 0.5445\n","Epoch 1 Loss 2.5817 Accuracy 0.5452\n","Epoch 1 Loss 2.5756 Accuracy 0.5459\n","Epoch 1 Loss 2.5692 Accuracy 0.5466\n","Epoch 1 Loss 2.5629 Accuracy 0.5474\n","Epoch 1 Loss 2.5566 Accuracy 0.5481\n","Epoch 1 Loss 2.5503 Accuracy 0.5488\n","Epoch 1 Loss 2.5442 Accuracy 0.5495\n","Epoch 1 Loss 2.5380 Accuracy 0.5502\n","Epoch 1 Loss 2.5319 Accuracy 0.5509\n","Epoch 1 Loss 2.5260 Accuracy 0.5516\n","Epoch 1 Loss 2.5201 Accuracy 0.5523\n","Epoch 1 Loss 2.5142 Accuracy 0.5530\n","Epoch 1 Loss 2.5083 Accuracy 0.5537\n","Epoch 1 Loss 2.5024 Accuracy 0.5544\n","Epoch 1 Loss 2.4966 Accuracy 0.5550\n","Epoch 1 Loss 2.4908 Accuracy 0.5557\n","Epoch 1 Loss 2.4852 Accuracy 0.5564\n","Epoch 1 Loss 2.4797 Accuracy 0.5570\n","Epoch 1 Loss 2.4740 Accuracy 0.5576\n","Epoch 1 Loss 2.4686 Accuracy 0.5583\n","Epoch 1 Loss 2.4631 Accuracy 0.5589\n","Epoch 1 Loss 2.4576 Accuracy 0.5595\n","Epoch 1 Loss 2.4521 Accuracy 0.5602\n","Epoch 1 Loss 2.4468 Accuracy 0.5608\n","Epoch 1 Loss 2.4414 Accuracy 0.5614\n","Epoch 1 Loss 2.4361 Accuracy 0.5620\n","Epoch 1 Loss 2.4308 Accuracy 0.5627\n","Epoch 1 Loss 2.4255 Accuracy 0.5633\n","Epoch 1 Loss 2.4203 Accuracy 0.5639\n","Epoch 1 Loss 2.4153 Accuracy 0.5645\n","Epoch 1 Loss 2.4103 Accuracy 0.5650\n","Epoch 1 Loss 2.4054 Accuracy 0.5656\n","Epoch 1 Loss 2.4003 Accuracy 0.5662\n","Epoch 1 Loss 2.3954 Accuracy 0.5668\n","Epoch 1 Loss 2.3904 Accuracy 0.5673\n","Epoch 1 Loss 2.3854 Accuracy 0.5679\n","Epoch 1 Loss 2.3806 Accuracy 0.5685\n","Epoch 1 Loss 2.3757 Accuracy 0.5691\n","Epoch 1 Loss 2.3709 Accuracy 0.5696\n","Epoch 1 Loss 2.3663 Accuracy 0.5702\n","Epoch 1 Loss 2.3615 Accuracy 0.5707\n","Epoch 1 Loss 2.3568 Accuracy 0.5713\n","Epoch 1 Loss 2.3519 Accuracy 0.5718\n","Epoch 1 Loss 2.3473 Accuracy 0.5724\n","Epoch 1 Loss 2.3426 Accuracy 0.5729\n","Epoch 1 Loss 2.3381 Accuracy 0.5735\n","Epoch 1 Loss 2.3334 Accuracy 0.5740\n","Epoch 1 Loss 2.3290 Accuracy 0.5745\n","Epoch 1 Loss 2.3245 Accuracy 0.5751\n","Epoch 1 Loss 2.3200 Accuracy 0.5756\n","Epoch 1 Loss 2.3156 Accuracy 0.5761\n","Epoch 1 Loss 2.3112 Accuracy 0.5766\n","Epoch 1 Loss 2.3068 Accuracy 0.5771\n","Epoch 1 Loss 2.3025 Accuracy 0.5776\n","Epoch 1 Loss 2.2981 Accuracy 0.5782\n","Epoch 1 Loss 2.2938 Accuracy 0.5787\n","Epoch 1 Loss 2.2895 Accuracy 0.5792\n","Epoch 1 Loss 2.2853 Accuracy 0.5797\n","Epoch 1 Loss 2.2810 Accuracy 0.5802\n","Epoch 1 Loss 2.2768 Accuracy 0.5807\n","Epoch 1 Loss 2.2725 Accuracy 0.5812\n","Epoch 1 Loss 2.2684 Accuracy 0.5817\n","Epoch 1 Loss 2.2643 Accuracy 0.5822\n","Epoch 1 Loss 2.2601 Accuracy 0.5827\n","Epoch 1 Loss 2.2561 Accuracy 0.5832\n","Epoch 1 Loss 2.2520 Accuracy 0.5836\n","Epoch 1 Loss 2.2480 Accuracy 0.5841\n","Epoch 1 Loss 2.2440 Accuracy 0.5846\n","Epoch 1 Loss 2.2401 Accuracy 0.5850\n","Epoch 1 Loss 2.2361 Accuracy 0.5855\n","Epoch 1 Loss 2.2323 Accuracy 0.5860\n","Epoch 1 Loss 2.2284 Accuracy 0.5864\n","Epoch 1 Loss 2.2245 Accuracy 0.5869\n","Epoch 1 Loss 2.2206 Accuracy 0.5874\n","Epoch 1 Loss 2.2168 Accuracy 0.5878\n","Epoch 1 Loss 2.2130 Accuracy 0.5883\n","Epoch 1 Loss 2.2092 Accuracy 0.5887\n","Epoch 1 Loss 2.2055 Accuracy 0.5892\n","Epoch 1 Loss 2.2019 Accuracy 0.5896\n","Epoch 1 Loss 2.1982 Accuracy 0.5900\n","Epoch 1 Loss 2.1945 Accuracy 0.5905\n","Epoch 1 Loss 2.1910 Accuracy 0.5909\n","Epoch 1 Loss 2.1872 Accuracy 0.5913\n","Epoch 1 Loss 2.1836 Accuracy 0.5918\n","Epoch 1 Loss 2.1801 Accuracy 0.5922\n","Epoch 1 Loss 2.1765 Accuracy 0.5926\n","Epoch 1 Loss 2.1729 Accuracy 0.5931\n","Epoch 1 Loss 2.1693 Accuracy 0.5935\n","Epoch 1 Loss 2.1659 Accuracy 0.5939\n","Epoch 1 Loss 2.1623 Accuracy 0.5944\n","Epoch 1 Loss 2.1589 Accuracy 0.5948\n","Epoch 1 Loss 2.1555 Accuracy 0.5952\n","Epoch 1 Loss 2.1520 Accuracy 0.5956\n","Epoch 1 Loss 2.1486 Accuracy 0.5960\n","Epoch 1 Loss 2.1452 Accuracy 0.5964\n","Epoch 1 Loss 2.1418 Accuracy 0.5968\n","Epoch 1 Loss 2.1384 Accuracy 0.5972\n","Epoch 1 Loss 2.1350 Accuracy 0.5977\n","Epoch 1 Loss 2.1317 Accuracy 0.5981\n","Epoch 1 Loss 2.1285 Accuracy 0.5984\n","Epoch 1 Loss 2.1252 Accuracy 0.5989\n","Epoch 1 Loss 2.1219 Accuracy 0.5992\n","Epoch 1 Loss 2.1187 Accuracy 0.5996\n","Epoch 1 Loss 2.1155 Accuracy 0.6000\n","Epoch 1 Loss 2.1123 Accuracy 0.6004\n","Epoch 1 Loss 2.1091 Accuracy 0.6008\n","Epoch 1 Loss 2.1059 Accuracy 0.6012\n","Epoch 1 Loss 2.1026 Accuracy 0.6016\n","Epoch 1 Loss 2.0995 Accuracy 0.6020\n","Epoch 1 Loss 2.0963 Accuracy 0.6024\n","Epoch 1 Loss 2.0933 Accuracy 0.6027\n","Epoch 1 Loss 2.0901 Accuracy 0.6031\n","Epoch 1 Loss 2.0871 Accuracy 0.6035\n","Epoch 1 Loss 2.0840 Accuracy 0.6039\n","Epoch 1 Loss 2.0810 Accuracy 0.6042\n","Epoch 1 Loss 2.0779 Accuracy 0.6046\n","Epoch 1 Loss 2.0749 Accuracy 0.6050\n","Epoch 1 Loss 2.0719 Accuracy 0.6054\n","Epoch 1 Loss 2.0690 Accuracy 0.6057\n","Epoch 1 Loss 2.0660 Accuracy 0.6061\n","Epoch 1 Loss 2.0630 Accuracy 0.6064\n","Epoch 1 Loss 2.0601 Accuracy 0.6068\n","Epoch 1 Loss 2.0572 Accuracy 0.6072\n","Epoch 1 Loss 2.0543 Accuracy 0.6075\n","Epoch 1 Loss 2.0514 Accuracy 0.6079\n","Epoch 1 Loss 2.0485 Accuracy 0.6082\n","Epoch 1 Loss 2.0457 Accuracy 0.6086\n","Epoch 1 Loss 2.0429 Accuracy 0.6089\n","Epoch 1 Loss 2.0401 Accuracy 0.6092\n","Epoch 1 Loss 2.0373 Accuracy 0.6096\n","Epoch 1 Loss 2.0344 Accuracy 0.6099\n","Epoch 1 Loss 2.0316 Accuracy 0.6103\n","Epoch 1 Loss 2.0287 Accuracy 0.6106\n","Epoch 1 Loss 2.0260 Accuracy 0.6110\n","Epoch 1 Loss 2.0232 Accuracy 0.6113\n","Epoch 1 Loss 2.0205 Accuracy 0.6116\n","Epoch 1 Loss 2.0177 Accuracy 0.6120\n","Epoch 1 Loss 2.0150 Accuracy 0.6123\n","Epoch 1 Loss 2.0123 Accuracy 0.6127\n","Epoch 1 Loss 2.0096 Accuracy 0.6130\n","Epoch 1 Loss 2.0069 Accuracy 0.6133\n","Epoch 1 Loss 2.0042 Accuracy 0.6137\n","Epoch 1 Loss 2.0016 Accuracy 0.6140\n","Epoch 1 Loss 1.9989 Accuracy 0.6143\n","Epoch 1 Loss 1.9964 Accuracy 0.6146\n","Epoch 1 Loss 1.9938 Accuracy 0.6149\n","Epoch 1 Loss 1.9912 Accuracy 0.6153\n","Epoch 1 Loss 1.9885 Accuracy 0.6156\n","Epoch 1 Loss 1.9860 Accuracy 0.6159\n","Epoch 1 Loss 1.9833 Accuracy 0.6162\n","Epoch 1 Loss 1.9808 Accuracy 0.6165\n","Epoch 1 Loss 1.9783 Accuracy 0.6168\n","Epoch 1 Loss 1.9758 Accuracy 0.6172\n","Epoch 1 Loss 1.9733 Accuracy 0.6175\n","Epoch 1 Loss 1.9709 Accuracy 0.6178\n","Epoch 1 Loss 1.9684 Accuracy 0.6181\n","Epoch 1 Loss 1.9658 Accuracy 0.6184\n","Epoch 1 Loss 1.9634 Accuracy 0.6187\n","Epoch 1 Loss 1.9610 Accuracy 0.6190\n","Epoch 1 Loss 1.9585 Accuracy 0.6193\n","Epoch 1 Loss 1.9561 Accuracy 0.6196\n","Epoch 1 Loss 1.9536 Accuracy 0.6199\n","Epoch 1 Loss 1.9512 Accuracy 0.6202\n","Epoch 1 Loss 1.9488 Accuracy 0.6205\n","Epoch 1 Loss 1.9464 Accuracy 0.6208\n","Epoch 1 Loss 1.9440 Accuracy 0.6211\n","Epoch 1 Loss 1.9417 Accuracy 0.6214\n","Epoch 1 Loss 1.9393 Accuracy 0.6217\n","Epoch 1 Loss 1.9370 Accuracy 0.6220\n","Epoch 1 Loss 1.9346 Accuracy 0.6223\n","Epoch 1 Loss 1.9324 Accuracy 0.6226\n","Epoch 1 Loss 1.9300 Accuracy 0.6229\n","Epoch 1 Loss 1.9278 Accuracy 0.6231\n","Epoch 1 Loss 1.9255 Accuracy 0.6234\n","Epoch 1 Loss 1.9231 Accuracy 0.6237\n","Epoch 1 Loss 1.9208 Accuracy 0.6240\n","Epoch 1 Loss 1.9185 Accuracy 0.6243\n","Epoch 1 Loss 1.9163 Accuracy 0.6246\n","Epoch 1 Loss 1.9141 Accuracy 0.6248\n","Epoch 1 Loss 1.9118 Accuracy 0.6251\n","Epoch 1 Loss 1.9097 Accuracy 0.6254\n","Epoch 1 Loss 1.9074 Accuracy 0.6257\n","Epoch 1 Loss 1.9052 Accuracy 0.6260\n","Epoch 1 Loss 1.9030 Accuracy 0.6262\n","Epoch 1 Loss 1.9009 Accuracy 0.6265\n","Epoch 1 Loss 1.8987 Accuracy 0.6268\n","Epoch 1 Loss 1.8965 Accuracy 0.6271\n","Epoch 1 Loss 1.8943 Accuracy 0.6273\n","Epoch 1 Loss 1.8921 Accuracy 0.6276\n","Epoch 1 Loss 1.8900 Accuracy 0.6279\n","Epoch 1 Loss 1.8879 Accuracy 0.6281\n","Epoch 1 Loss 1.8858 Accuracy 0.6284\n","Epoch 1 Loss 1.8836 Accuracy 0.6287\n","Epoch 1 Loss 1.8815 Accuracy 0.6289\n","Epoch 1 Loss 1.8794 Accuracy 0.6292\n","Epoch 1 Loss 1.8772 Accuracy 0.6295\n","Epoch 1 Loss 1.8752 Accuracy 0.6297\n","Epoch 1 Loss 1.8731 Accuracy 0.6300\n","Epoch 1 Loss 1.8711 Accuracy 0.6302\n","Epoch 1 Loss 1.8690 Accuracy 0.6305\n","Epoch 1 Loss 1.8670 Accuracy 0.6308\n","Epoch 1 Loss 1.8650 Accuracy 0.6310\n","Epoch 1 Loss 1.8629 Accuracy 0.6313\n","Epoch 1 Loss 1.8609 Accuracy 0.6315\n","Epoch 1 Loss 1.8589 Accuracy 0.6318\n","Epoch 1 Loss 1.8569 Accuracy 0.6320\n","Epoch 1 Loss 1.8549 Accuracy 0.6323\n","Epoch 1 Loss 1.8529 Accuracy 0.6325\n","Epoch 1 Loss 1.8510 Accuracy 0.6328\n","Epoch 1 Loss 1.8490 Accuracy 0.6330\n","Epoch 1 Loss 1.8471 Accuracy 0.6333\n","Epoch 1 Loss 1.8451 Accuracy 0.6335\n","Epoch 1 Loss 1.8432 Accuracy 0.6337\n","Epoch 1 Loss 1.8413 Accuracy 0.6340\n","Epoch 1 Loss 1.8393 Accuracy 0.6342\n","Epoch 1 Loss 1.8374 Accuracy 0.6345\n","Epoch 1 Loss 1.8355 Accuracy 0.6347\n","Epoch 1 Loss 1.8335 Accuracy 0.6350\n","Epoch 1 Loss 1.8316 Accuracy 0.6352\n","Epoch 1 Loss 1.8297 Accuracy 0.6355\n","Epoch 1 Loss 1.8279 Accuracy 0.6357\n","Epoch 1 Loss 1.8260 Accuracy 0.6359\n","Epoch 1 Loss 1.8242 Accuracy 0.6362\n","Epoch 1 Loss 1.8223 Accuracy 0.6364\n","Epoch 1 Loss 1.8205 Accuracy 0.6366\n","Epoch 1 Loss 1.8187 Accuracy 0.6369\n","Epoch 1 Loss 1.8169 Accuracy 0.6371\n","Epoch 1 Loss 1.8150 Accuracy 0.6373\n","Epoch 1 Loss 1.8132 Accuracy 0.6376\n","Epoch 1 Loss 1.8113 Accuracy 0.6378\n","Epoch 1 Loss 1.8095 Accuracy 0.6380\n","Epoch 1 Loss 1.8077 Accuracy 0.6383\n","Epoch 1 Loss 1.8059 Accuracy 0.6385\n","Epoch 1 Loss 1.8041 Accuracy 0.6387\n","Epoch 1 Loss 1.8023 Accuracy 0.6390\n","Epoch 1 Loss 1.8005 Accuracy 0.6392\n","Epoch 1 Loss 1.7987 Accuracy 0.6394\n","Epoch 1 Loss 1.7970 Accuracy 0.6396\n","Epoch 1 Loss 1.7953 Accuracy 0.6398\n","Epoch 1 Loss 1.7935 Accuracy 0.6401\n","Epoch 1 Loss 1.7918 Accuracy 0.6403\n","Epoch 1 Loss 1.7900 Accuracy 0.6405\n","Epoch 1 Loss 1.7883 Accuracy 0.6407\n","Epoch 1 Loss 1.7866 Accuracy 0.6409\n","Epoch 1 Loss 1.7849 Accuracy 0.6412\n","Epoch 1 Loss 1.7832 Accuracy 0.6414\n","Epoch 1 Loss 1.7814 Accuracy 0.6416\n","Epoch 1 Loss 1.7798 Accuracy 0.6418\n","Epoch 1 Loss 1.7780 Accuracy 0.6420\n","Epoch 1 Loss 1.7763 Accuracy 0.6423\n","Epoch 1 Loss 1.7747 Accuracy 0.6425\n","Epoch 1 Loss 1.7730 Accuracy 0.6427\n","Epoch 1 Loss 1.7713 Accuracy 0.6429\n","Epoch 1 Loss 1.7696 Accuracy 0.6431\n","Epoch 1 Loss 1.7679 Accuracy 0.6434\n","Epoch 1 Loss 1.7663 Accuracy 0.6436\n","Epoch 1 Loss 1.7646 Accuracy 0.6438\n","Epoch 1 Loss 1.7630 Accuracy 0.6440\n","Epoch 1 Loss 1.7613 Accuracy 0.6442\n","Epoch 1 Loss 1.7597 Accuracy 0.6444\n","Epoch 1 Loss 1.7581 Accuracy 0.6446\n","Epoch 1 Loss 1.7564 Accuracy 0.6448\n","Epoch 1 Loss 1.7548 Accuracy 0.6450\n","Epoch 1 Loss 1.7533 Accuracy 0.6452\n","Epoch 1 Loss 1.7517 Accuracy 0.6454\n","Epoch 1 Loss 1.7501 Accuracy 0.6457\n","Epoch 1 Loss 1.7485 Accuracy 0.6459\n","Epoch 1 Loss 1.7469 Accuracy 0.6461\n","Epoch 1 Loss 1.7453 Accuracy 0.6463\n","Epoch 1 Loss 1.7437 Accuracy 0.6465\n","Epoch 1 Loss 1.7421 Accuracy 0.6467\n","Epoch 1 Loss 1.7405 Accuracy 0.6469\n","Epoch 1 Loss 1.7390 Accuracy 0.6471\n","Epoch 1 Loss 1.7375 Accuracy 0.6473\n","Epoch 1 Loss 1.7360 Accuracy 0.6475\n","Epoch 1 Loss 1.7344 Accuracy 0.6477\n","Epoch 1 Loss 1.7329 Accuracy 0.6479\n","Epoch 1 Loss 1.7314 Accuracy 0.6481\n","Epoch 1 Loss 1.7299 Accuracy 0.6483\n","Epoch 1 Loss 1.7283 Accuracy 0.6485\n","Epoch 1 Loss 1.7269 Accuracy 0.6487\n","Epoch 1 Loss 1.7253 Accuracy 0.6489\n","Epoch 1 Loss 1.7238 Accuracy 0.6490\n","Epoch 1 Loss 1.7223 Accuracy 0.6492\n","Epoch 1 Loss 1.7208 Accuracy 0.6494\n","Epoch 1 Loss 1.7193 Accuracy 0.6496\n","Epoch 1 Loss 1.7178 Accuracy 0.6498\n","Epoch 1 Loss 1.7163 Accuracy 0.6500\n","Epoch 1 Loss 1.7148 Accuracy 0.6502\n","Epoch 1 Loss 1.7133 Accuracy 0.6504\n","Epoch 1 Loss 1.7118 Accuracy 0.6506\n","Epoch 1 Loss 1.7103 Accuracy 0.6508\n","Epoch 1 Loss 1.7089 Accuracy 0.6510\n","Epoch 1 Loss 1.7074 Accuracy 0.6512\n","Epoch 1 Loss 1.7060 Accuracy 0.6514\n","Epoch 1 Loss 1.7046 Accuracy 0.6516\n","Epoch 1 Loss 1.7031 Accuracy 0.6517\n","Epoch 1 Loss 1.7017 Accuracy 0.6519\n","Epoch 1 Loss 1.7003 Accuracy 0.6521\n","Epoch 1 Loss 1.6989 Accuracy 0.6523\n","Epoch 1 Loss 1.6975 Accuracy 0.6525\n","Epoch 1 Loss 1.6961 Accuracy 0.6527\n","Epoch 1 Loss 1.6947 Accuracy 0.6528\n","Epoch 1 Loss 1.6933 Accuracy 0.6530\n","Epoch 1 Loss 1.6919 Accuracy 0.6532\n","Epoch 1 Loss 1.6906 Accuracy 0.6534\n","Epoch 1 Loss 1.6892 Accuracy 0.6535\n","Epoch 1 Loss 1.6879 Accuracy 0.6537\n","Epoch 1 Loss 1.6865 Accuracy 0.6539\n","Epoch 1 Loss 1.6851 Accuracy 0.6541\n","Epoch 1 Loss 1.6838 Accuracy 0.6542\n","Epoch 1 Loss 1.6823 Accuracy 0.6544\n","Epoch 1 Loss 1.6810 Accuracy 0.6546\n","Epoch 1 Loss 1.6796 Accuracy 0.6548\n","Epoch 1 Loss 1.6783 Accuracy 0.6550\n","Epoch 1 Loss 1.6770 Accuracy 0.6551\n","Epoch 1 Loss 1.6756 Accuracy 0.6553\n","Epoch 1 Loss 1.6743 Accuracy 0.6555\n","Epoch 1 Loss 1.6730 Accuracy 0.6557\n","Epoch 1 Loss 1.6716 Accuracy 0.6558\n","Epoch 1 Loss 1.6703 Accuracy 0.6560\n","Epoch 1 Loss 1.6690 Accuracy 0.6562\n","Epoch 1 Loss 1.6676 Accuracy 0.6564\n","Epoch 1 Loss 1.6663 Accuracy 0.6565\n","Epoch 1 Loss 1.6650 Accuracy 0.6567\n","Epoch 1 Loss 1.6637 Accuracy 0.6569\n","Epoch 1 Loss 1.6624 Accuracy 0.6570\n","Epoch 1 Loss 1.6611 Accuracy 0.6572\n","Epoch 1 Loss 1.6598 Accuracy 0.6574\n","Epoch 1 Loss 1.6585 Accuracy 0.6575\n","Epoch 1 Loss 1.6573 Accuracy 0.6577\n","Epoch 1 Loss 1.6560 Accuracy 0.6579\n","Epoch 1 Loss 1.6547 Accuracy 0.6580\n","Epoch 1 Loss 1.6535 Accuracy 0.6582\n","Epoch 1 Loss 1.6522 Accuracy 0.6584\n","Epoch 1 Loss 1.6509 Accuracy 0.6586\n","Epoch 1 Loss 1.6497 Accuracy 0.6587\n","Epoch 1 Loss 1.6484 Accuracy 0.6589\n","Epoch 1 Loss 1.6472 Accuracy 0.6590\n","Epoch 1 Loss 1.6460 Accuracy 0.6592\n","Epoch 1 Loss 1.6447 Accuracy 0.6594\n","Epoch 1 Loss 1.6435 Accuracy 0.6595\n","Epoch 1 Loss 1.6422 Accuracy 0.6597\n","Epoch 1 Loss 1.6410 Accuracy 0.6599\n","Epoch 1 Loss 1.6398 Accuracy 0.6600\n","Epoch 1 Loss 1.6386 Accuracy 0.6602\n","Epoch 1 Loss 1.6374 Accuracy 0.6603\n","Epoch 1 Loss 1.6362 Accuracy 0.6605\n","Epoch 1 Loss 1.6350 Accuracy 0.6606\n","Epoch 1 Loss 1.6338 Accuracy 0.6608\n","Epoch 1 Loss 1.6326 Accuracy 0.6610\n","Epoch 1 Loss 1.6314 Accuracy 0.6611\n","Epoch 1 Loss 1.6301 Accuracy 0.6613\n","Epoch 1 Loss 1.6290 Accuracy 0.6614\n","Epoch 1 Loss 1.6278 Accuracy 0.6616\n","Epoch 1 Loss 1.6266 Accuracy 0.6618\n","Epoch 1 Loss 1.6254 Accuracy 0.6619\n","Epoch 1 Loss 1.6242 Accuracy 0.6621\n","Epoch 1 Loss 1.6230 Accuracy 0.6622\n","Epoch 1 Loss 1.6219 Accuracy 0.6624\n","Epoch 1 Loss 1.6207 Accuracy 0.6625\n","Epoch 1 Loss 1.6195 Accuracy 0.6627\n","Epoch 1 Loss 1.6184 Accuracy 0.6628\n","Epoch 1 Loss 1.6172 Accuracy 0.6630\n","Epoch 1 Loss 1.6160 Accuracy 0.6632\n","Epoch 1 Loss 1.6149 Accuracy 0.6633\n","Epoch 1 Loss 1.6137 Accuracy 0.6635\n","Epoch 1 Loss 1.6125 Accuracy 0.6636\n","Epoch 1 Loss 1.6114 Accuracy 0.6638\n","Epoch 1 Loss 1.6103 Accuracy 0.6639\n","Epoch 1 Loss 1.6091 Accuracy 0.6641\n","Epoch 1 Loss 1.6080 Accuracy 0.6642\n","Epoch 1 Loss 1.6069 Accuracy 0.6644\n","Epoch 1 Loss 1.6058 Accuracy 0.6645\n","Epoch 1 Loss 1.6047 Accuracy 0.6647\n","Epoch 1 Loss 1.6035 Accuracy 0.6648\n","Epoch 1 Loss 1.6024 Accuracy 0.6650\n","Epoch 1 Loss 1.6013 Accuracy 0.6651\n","Epoch 1 Loss 1.6002 Accuracy 0.6653\n","Epoch 1 Loss 1.5991 Accuracy 0.6654\n","Epoch 1 Loss 1.5980 Accuracy 0.6655\n","Epoch 1 Loss 1.5969 Accuracy 0.6657\n","Epoch 1 Loss 1.5958 Accuracy 0.6658\n","Epoch 1 Loss 1.5947 Accuracy 0.6660\n","Epoch 1 Loss 1.5936 Accuracy 0.6661\n","Epoch 1 Loss 1.5925 Accuracy 0.6663\n","Epoch 1 Loss 1.5914 Accuracy 0.6664\n","Epoch 1 Loss 1.5903 Accuracy 0.6666\n","Epoch 1 Loss 1.5893 Accuracy 0.6667\n","Epoch 1 Loss 1.5882 Accuracy 0.6668\n","Epoch 1 Loss 1.5871 Accuracy 0.6670\n","Epoch 1 Loss 1.5861 Accuracy 0.6671\n","Epoch 1 Loss 1.5850 Accuracy 0.6673\n","Epoch 1 Loss 1.5839 Accuracy 0.6674\n","Epoch 1 Loss 1.5828 Accuracy 0.6676\n","Epoch 1 Loss 1.5818 Accuracy 0.6677\n","Epoch 1 Loss 1.5807 Accuracy 0.6678\n","Epoch 1 Loss 1.5797 Accuracy 0.6680\n","Epoch 1 Loss 1.5786 Accuracy 0.6681\n","Epoch 1 Loss 1.5776 Accuracy 0.6683\n","Epoch 1 Loss 1.5765 Accuracy 0.6684\n","Epoch 1 Loss 1.5755 Accuracy 0.6685\n","Epoch 1 Loss 1.5744 Accuracy 0.6687\n","Epoch 1 Loss 1.5734 Accuracy 0.6688\n","Epoch 1 Loss 1.5724 Accuracy 0.6690\n","Epoch 1 Loss 1.5713 Accuracy 0.6691\n","Epoch 1 Loss 1.5703 Accuracy 0.6692\n","Epoch 1 Loss 1.5693 Accuracy 0.6694\n","Epoch 1 Loss 1.5682 Accuracy 0.6695\n","Epoch 1 Loss 1.5672 Accuracy 0.6697\n","Epoch 1 Loss 1.5662 Accuracy 0.6698\n","Epoch 1 Loss 1.5652 Accuracy 0.6699\n","Epoch 1 Loss 1.5642 Accuracy 0.6701\n","Epoch 1 Loss 1.5632 Accuracy 0.6702\n","Epoch 1 Loss 1.5622 Accuracy 0.6703\n","Epoch 1 Loss 1.5612 Accuracy 0.6705\n","Epoch 1 Loss 1.5602 Accuracy 0.6706\n","Epoch 1 Loss 1.5592 Accuracy 0.6707\n","Epoch 1 Loss 1.5582 Accuracy 0.6709\n","Epoch 1 Loss 1.5572 Accuracy 0.6710\n","Epoch 1 Loss 1.5561 Accuracy 0.6711\n","Saving checkpoint for epoch 1 at ./ckpt/ckpt-1\n","Epoch 1 Loss 1.5561 Accuracy 0.6711\n","Time taken for 1 epoch: 18791.313880205154 secs\n","\n","Epoch 2 Loss 0.8507 Accuracy 0.7667\n","Epoch 2 Loss 0.9444 Accuracy 0.7522\n","Epoch 2 Loss 0.9256 Accuracy 0.7564\n","Epoch 2 Loss 0.9200 Accuracy 0.7578\n","Epoch 2 Loss 0.9236 Accuracy 0.7571\n","Epoch 2 Loss 0.9229 Accuracy 0.7570\n","Epoch 2 Loss 0.9211 Accuracy 0.7574\n","Epoch 2 Loss 0.9235 Accuracy 0.7572\n","Epoch 2 Loss 0.9207 Accuracy 0.7579\n","Epoch 2 Loss 0.9209 Accuracy 0.7578\n","Epoch 2 Loss 0.9225 Accuracy 0.7576\n","Epoch 2 Loss 0.9218 Accuracy 0.7579\n","Epoch 2 Loss 0.9215 Accuracy 0.7578\n","Epoch 2 Loss 0.9231 Accuracy 0.7575\n","Epoch 2 Loss 0.9219 Accuracy 0.7576\n","Epoch 2 Loss 0.9220 Accuracy 0.7576\n","Epoch 2 Loss 0.9226 Accuracy 0.7574\n","Epoch 2 Loss 0.9225 Accuracy 0.7575\n","Epoch 2 Loss 0.9226 Accuracy 0.7574\n","Epoch 2 Loss 0.9219 Accuracy 0.7575\n","Epoch 2 Loss 0.9214 Accuracy 0.7577\n","Epoch 2 Loss 0.9210 Accuracy 0.7577\n","Epoch 2 Loss 0.9211 Accuracy 0.7577\n","Epoch 2 Loss 0.9211 Accuracy 0.7577\n","Epoch 2 Loss 0.9204 Accuracy 0.7578\n","Epoch 2 Loss 0.9205 Accuracy 0.7578\n","Epoch 2 Loss 0.9202 Accuracy 0.7579\n","Epoch 2 Loss 0.9203 Accuracy 0.7578\n","Epoch 2 Loss 0.9203 Accuracy 0.7578\n","Epoch 2 Loss 0.9206 Accuracy 0.7578\n","Epoch 2 Loss 0.9203 Accuracy 0.7578\n","Epoch 2 Loss 0.9206 Accuracy 0.7577\n","Epoch 2 Loss 0.9206 Accuracy 0.7577\n","Epoch 2 Loss 0.9206 Accuracy 0.7577\n","Epoch 2 Loss 0.9211 Accuracy 0.7577\n","Epoch 2 Loss 0.9210 Accuracy 0.7577\n","Epoch 2 Loss 0.9206 Accuracy 0.7577\n","Epoch 2 Loss 0.9212 Accuracy 0.7576\n","Epoch 2 Loss 0.9208 Accuracy 0.7577\n","Epoch 2 Loss 0.9211 Accuracy 0.7576\n","Epoch 2 Loss 0.9209 Accuracy 0.7577\n","Epoch 2 Loss 0.9211 Accuracy 0.7577\n","Epoch 2 Loss 0.9206 Accuracy 0.7577\n","Epoch 2 Loss 0.9200 Accuracy 0.7579\n","Epoch 2 Loss 0.9200 Accuracy 0.7579\n","Epoch 2 Loss 0.9200 Accuracy 0.7579\n","Epoch 2 Loss 0.9204 Accuracy 0.7578\n","Epoch 2 Loss 0.9203 Accuracy 0.7578\n","Epoch 2 Loss 0.9204 Accuracy 0.7578\n","Epoch 2 Loss 0.9201 Accuracy 0.7578\n","Epoch 2 Loss 0.9200 Accuracy 0.7578\n","Epoch 2 Loss 0.9198 Accuracy 0.7579\n","Epoch 2 Loss 0.9197 Accuracy 0.7579\n","Epoch 2 Loss 0.9198 Accuracy 0.7579\n","Epoch 2 Loss 0.9200 Accuracy 0.7578\n","Epoch 2 Loss 0.9199 Accuracy 0.7578\n","Epoch 2 Loss 0.9201 Accuracy 0.7578\n","Epoch 2 Loss 0.9198 Accuracy 0.7578\n","Epoch 2 Loss 0.9197 Accuracy 0.7578\n","Epoch 2 Loss 0.9193 Accuracy 0.7579\n","Epoch 2 Loss 0.9191 Accuracy 0.7579\n","Epoch 2 Loss 0.9192 Accuracy 0.7579\n","Epoch 2 Loss 0.9189 Accuracy 0.7579\n","Epoch 2 Loss 0.9189 Accuracy 0.7579\n","Epoch 2 Loss 0.9190 Accuracy 0.7579\n","Epoch 2 Loss 0.9189 Accuracy 0.7580\n","Epoch 2 Loss 0.9190 Accuracy 0.7579\n","Epoch 2 Loss 0.9189 Accuracy 0.7579\n","Epoch 2 Loss 0.9189 Accuracy 0.7579\n","Epoch 2 Loss 0.9187 Accuracy 0.7580\n","Epoch 2 Loss 0.9186 Accuracy 0.7580\n","Epoch 2 Loss 0.9188 Accuracy 0.7579\n","Epoch 2 Loss 0.9187 Accuracy 0.7580\n","Epoch 2 Loss 0.9186 Accuracy 0.7580\n","Epoch 2 Loss 0.9185 Accuracy 0.7580\n","Epoch 2 Loss 0.9188 Accuracy 0.7579\n","Epoch 2 Loss 0.9189 Accuracy 0.7579\n","Epoch 2 Loss 0.9188 Accuracy 0.7579\n","Epoch 2 Loss 0.9186 Accuracy 0.7580\n","Epoch 2 Loss 0.9187 Accuracy 0.7579\n","Epoch 2 Loss 0.9187 Accuracy 0.7579\n","Epoch 2 Loss 0.9186 Accuracy 0.7579\n","Epoch 2 Loss 0.9184 Accuracy 0.7580\n","Epoch 2 Loss 0.9183 Accuracy 0.7580\n","Epoch 2 Loss 0.9183 Accuracy 0.7579\n","Epoch 2 Loss 0.9183 Accuracy 0.7580\n","Epoch 2 Loss 0.9181 Accuracy 0.7580\n","Epoch 2 Loss 0.9181 Accuracy 0.7580\n","Epoch 2 Loss 0.9180 Accuracy 0.7580\n","Epoch 2 Loss 0.9178 Accuracy 0.7580\n","Epoch 2 Loss 0.9178 Accuracy 0.7580\n","Epoch 2 Loss 0.9177 Accuracy 0.7581\n","Epoch 2 Loss 0.9175 Accuracy 0.7581\n","Epoch 2 Loss 0.9174 Accuracy 0.7581\n","Epoch 2 Loss 0.9171 Accuracy 0.7582\n","Epoch 2 Loss 0.9170 Accuracy 0.7582\n","Epoch 2 Loss 0.9169 Accuracy 0.7582\n","Epoch 2 Loss 0.9166 Accuracy 0.7583\n","Epoch 2 Loss 0.9164 Accuracy 0.7583\n","Epoch 2 Loss 0.9162 Accuracy 0.7583\n","Epoch 2 Loss 0.9162 Accuracy 0.7584\n","Epoch 2 Loss 0.9161 Accuracy 0.7584\n","Epoch 2 Loss 0.9160 Accuracy 0.7584\n","Epoch 2 Loss 0.9157 Accuracy 0.7584\n","Epoch 2 Loss 0.9158 Accuracy 0.7584\n","Epoch 2 Loss 0.9156 Accuracy 0.7585\n","Epoch 2 Loss 0.9155 Accuracy 0.7585\n","Epoch 2 Loss 0.9153 Accuracy 0.7585\n","Epoch 2 Loss 0.9151 Accuracy 0.7585\n","Epoch 2 Loss 0.9150 Accuracy 0.7585\n","Epoch 2 Loss 0.9150 Accuracy 0.7586\n","Epoch 2 Loss 0.9147 Accuracy 0.7586\n","Epoch 2 Loss 0.9145 Accuracy 0.7586\n","Epoch 2 Loss 0.9146 Accuracy 0.7586\n","Epoch 2 Loss 0.9144 Accuracy 0.7587\n","Epoch 2 Loss 0.9144 Accuracy 0.7587\n","Epoch 2 Loss 0.9144 Accuracy 0.7587\n","Epoch 2 Loss 0.9144 Accuracy 0.7587\n","Epoch 2 Loss 0.9143 Accuracy 0.7587\n","Epoch 2 Loss 0.9141 Accuracy 0.7587\n","Epoch 2 Loss 0.9140 Accuracy 0.7588\n","Epoch 2 Loss 0.9139 Accuracy 0.7588\n","Epoch 2 Loss 0.9139 Accuracy 0.7588\n","Epoch 2 Loss 0.9137 Accuracy 0.7588\n","Epoch 2 Loss 0.9136 Accuracy 0.7588\n","Epoch 2 Loss 0.9135 Accuracy 0.7588\n","Epoch 2 Loss 0.9134 Accuracy 0.7589\n","Epoch 2 Loss 0.9133 Accuracy 0.7589\n","Epoch 2 Loss 0.9131 Accuracy 0.7589\n","Epoch 2 Loss 0.9131 Accuracy 0.7589\n","Epoch 2 Loss 0.9130 Accuracy 0.7589\n","Epoch 2 Loss 0.9128 Accuracy 0.7590\n","Epoch 2 Loss 0.9127 Accuracy 0.7590\n","Epoch 2 Loss 0.9126 Accuracy 0.7590\n","Epoch 2 Loss 0.9126 Accuracy 0.7590\n","Epoch 2 Loss 0.9126 Accuracy 0.7590\n","Epoch 2 Loss 0.9126 Accuracy 0.7590\n","Epoch 2 Loss 0.9125 Accuracy 0.7590\n","Epoch 2 Loss 0.9124 Accuracy 0.7591\n","Epoch 2 Loss 0.9124 Accuracy 0.7591\n","Epoch 2 Loss 0.9122 Accuracy 0.7591\n","Epoch 2 Loss 0.9121 Accuracy 0.7591\n","Epoch 2 Loss 0.9121 Accuracy 0.7591\n","Epoch 2 Loss 0.9119 Accuracy 0.7591\n","Epoch 2 Loss 0.9117 Accuracy 0.7592\n","Epoch 2 Loss 0.9116 Accuracy 0.7592\n","Epoch 2 Loss 0.9115 Accuracy 0.7592\n","Epoch 2 Loss 0.9115 Accuracy 0.7592\n","Epoch 2 Loss 0.9114 Accuracy 0.7592\n","Epoch 2 Loss 0.9114 Accuracy 0.7592\n","Epoch 2 Loss 0.9114 Accuracy 0.7592\n","Epoch 2 Loss 0.9113 Accuracy 0.7592\n","Epoch 2 Loss 0.9113 Accuracy 0.7593\n","Epoch 2 Loss 0.9112 Accuracy 0.7593\n","Epoch 2 Loss 0.9110 Accuracy 0.7593\n","Epoch 2 Loss 0.9110 Accuracy 0.7593\n","Epoch 2 Loss 0.9108 Accuracy 0.7593\n","Epoch 2 Loss 0.9108 Accuracy 0.7594\n","Epoch 2 Loss 0.9108 Accuracy 0.7593\n","Epoch 2 Loss 0.9108 Accuracy 0.7594\n","Epoch 2 Loss 0.9105 Accuracy 0.7594\n","Epoch 2 Loss 0.9105 Accuracy 0.7594\n","Epoch 2 Loss 0.9104 Accuracy 0.7594\n","Epoch 2 Loss 0.9103 Accuracy 0.7594\n","Epoch 2 Loss 0.9101 Accuracy 0.7595\n","Epoch 2 Loss 0.9100 Accuracy 0.7595\n","Epoch 2 Loss 0.9099 Accuracy 0.7595\n","Epoch 2 Loss 0.9096 Accuracy 0.7596\n","Epoch 2 Loss 0.9094 Accuracy 0.7596\n","Epoch 2 Loss 0.9093 Accuracy 0.7597\n","Epoch 2 Loss 0.9092 Accuracy 0.7597\n","Epoch 2 Loss 0.9089 Accuracy 0.7597\n","Epoch 2 Loss 0.9088 Accuracy 0.7598\n","Epoch 2 Loss 0.9086 Accuracy 0.7598\n","Epoch 2 Loss 0.9085 Accuracy 0.7598\n","Epoch 2 Loss 0.9084 Accuracy 0.7598\n","Epoch 2 Loss 0.9083 Accuracy 0.7598\n","Epoch 2 Loss 0.9083 Accuracy 0.7598\n","Epoch 2 Loss 0.9081 Accuracy 0.7599\n","Epoch 2 Loss 0.9080 Accuracy 0.7599\n","Epoch 2 Loss 0.9078 Accuracy 0.7599\n","Epoch 2 Loss 0.9076 Accuracy 0.7600\n","Epoch 2 Loss 0.9075 Accuracy 0.7600\n","Epoch 2 Loss 0.9075 Accuracy 0.7600\n","Epoch 2 Loss 0.9074 Accuracy 0.7600\n","Epoch 2 Loss 0.9072 Accuracy 0.7601\n","Epoch 2 Loss 0.9069 Accuracy 0.7601\n","Epoch 2 Loss 0.9068 Accuracy 0.7601\n","Epoch 2 Loss 0.9067 Accuracy 0.7602\n","Epoch 2 Loss 0.9066 Accuracy 0.7602\n","Epoch 2 Loss 0.9065 Accuracy 0.7602\n","Epoch 2 Loss 0.9063 Accuracy 0.7602\n","Epoch 2 Loss 0.9063 Accuracy 0.7603\n","Epoch 2 Loss 0.9062 Accuracy 0.7603\n","Epoch 2 Loss 0.9061 Accuracy 0.7603\n","Epoch 2 Loss 0.9059 Accuracy 0.7603\n","Epoch 2 Loss 0.9058 Accuracy 0.7603\n","Epoch 2 Loss 0.9056 Accuracy 0.7604\n","Epoch 2 Loss 0.9055 Accuracy 0.7604\n","Epoch 2 Loss 0.9053 Accuracy 0.7605\n","Epoch 2 Loss 0.9052 Accuracy 0.7605\n","Epoch 2 Loss 0.9051 Accuracy 0.7605\n","Epoch 2 Loss 0.9050 Accuracy 0.7605\n","Epoch 2 Loss 0.9048 Accuracy 0.7605\n","Epoch 2 Loss 0.9047 Accuracy 0.7606\n","Epoch 2 Loss 0.9046 Accuracy 0.7606\n","Epoch 2 Loss 0.9044 Accuracy 0.7606\n","Epoch 2 Loss 0.9043 Accuracy 0.7606\n","Epoch 2 Loss 0.9042 Accuracy 0.7607\n","Epoch 2 Loss 0.9041 Accuracy 0.7607\n","Epoch 2 Loss 0.9040 Accuracy 0.7607\n","Epoch 2 Loss 0.9038 Accuracy 0.7607\n","Epoch 2 Loss 0.9036 Accuracy 0.7608\n","Epoch 2 Loss 0.9035 Accuracy 0.7608\n","Epoch 2 Loss 0.9034 Accuracy 0.7608\n","Epoch 2 Loss 0.9032 Accuracy 0.7609\n","Epoch 2 Loss 0.9031 Accuracy 0.7609\n","Epoch 2 Loss 0.9031 Accuracy 0.7609\n","Epoch 2 Loss 0.9030 Accuracy 0.7609\n","Epoch 2 Loss 0.9031 Accuracy 0.7609\n","Epoch 2 Loss 0.9030 Accuracy 0.7609\n","Epoch 2 Loss 0.9029 Accuracy 0.7609\n","Epoch 2 Loss 0.9027 Accuracy 0.7610\n","Epoch 2 Loss 0.9026 Accuracy 0.7610\n","Epoch 2 Loss 0.9025 Accuracy 0.7610\n","Epoch 2 Loss 0.9024 Accuracy 0.7610\n","Epoch 2 Loss 0.9022 Accuracy 0.7611\n","Epoch 2 Loss 0.9020 Accuracy 0.7611\n","Epoch 2 Loss 0.9019 Accuracy 0.7611\n","Epoch 2 Loss 0.9018 Accuracy 0.7611\n","Epoch 2 Loss 0.9017 Accuracy 0.7612\n","Epoch 2 Loss 0.9016 Accuracy 0.7612\n","Epoch 2 Loss 0.9016 Accuracy 0.7612\n","Epoch 2 Loss 0.9015 Accuracy 0.7612\n","Epoch 2 Loss 0.9014 Accuracy 0.7612\n","Epoch 2 Loss 0.9012 Accuracy 0.7613\n","Epoch 2 Loss 0.9011 Accuracy 0.7613\n","Epoch 2 Loss 0.9010 Accuracy 0.7613\n","Epoch 2 Loss 0.9009 Accuracy 0.7613\n","Epoch 2 Loss 0.9008 Accuracy 0.7613\n","Epoch 2 Loss 0.9007 Accuracy 0.7614\n","Epoch 2 Loss 0.9006 Accuracy 0.7614\n","Epoch 2 Loss 0.9006 Accuracy 0.7614\n","Epoch 2 Loss 0.9004 Accuracy 0.7614\n","Epoch 2 Loss 0.9003 Accuracy 0.7614\n","Epoch 2 Loss 0.9002 Accuracy 0.7615\n","Epoch 2 Loss 0.9001 Accuracy 0.7615\n","Epoch 2 Loss 0.9000 Accuracy 0.7615\n","Epoch 2 Loss 0.8998 Accuracy 0.7615\n","Epoch 2 Loss 0.8997 Accuracy 0.7616\n","Epoch 2 Loss 0.8996 Accuracy 0.7616\n","Epoch 2 Loss 0.8995 Accuracy 0.7616\n","Epoch 2 Loss 0.8993 Accuracy 0.7617\n","Epoch 2 Loss 0.8991 Accuracy 0.7617\n","Epoch 2 Loss 0.8991 Accuracy 0.7617\n","Epoch 2 Loss 0.8989 Accuracy 0.7617\n","Epoch 2 Loss 0.8989 Accuracy 0.7617\n","Epoch 2 Loss 0.8988 Accuracy 0.7618\n","Epoch 2 Loss 0.8987 Accuracy 0.7618\n","Epoch 2 Loss 0.8985 Accuracy 0.7618\n","Epoch 2 Loss 0.8984 Accuracy 0.7618\n","Epoch 2 Loss 0.8983 Accuracy 0.7619\n","Epoch 2 Loss 0.8981 Accuracy 0.7619\n","Epoch 2 Loss 0.8980 Accuracy 0.7619\n","Epoch 2 Loss 0.8979 Accuracy 0.7619\n","Epoch 2 Loss 0.8977 Accuracy 0.7619\n","Epoch 2 Loss 0.8977 Accuracy 0.7620\n","Epoch 2 Loss 0.8976 Accuracy 0.7620\n","Epoch 2 Loss 0.8975 Accuracy 0.7620\n","Epoch 2 Loss 0.8975 Accuracy 0.7620\n","Epoch 2 Loss 0.8974 Accuracy 0.7620\n","Epoch 2 Loss 0.8973 Accuracy 0.7620\n","Epoch 2 Loss 0.8971 Accuracy 0.7621\n","Epoch 2 Loss 0.8970 Accuracy 0.7621\n","Epoch 2 Loss 0.8970 Accuracy 0.7621\n","Epoch 2 Loss 0.8969 Accuracy 0.7621\n","Epoch 2 Loss 0.8968 Accuracy 0.7621\n","Epoch 2 Loss 0.8967 Accuracy 0.7621\n","Epoch 2 Loss 0.8965 Accuracy 0.7622\n","Epoch 2 Loss 0.8964 Accuracy 0.7622\n","Epoch 2 Loss 0.8963 Accuracy 0.7622\n","Epoch 2 Loss 0.8962 Accuracy 0.7622\n","Epoch 2 Loss 0.8960 Accuracy 0.7623\n","Epoch 2 Loss 0.8960 Accuracy 0.7623\n","Epoch 2 Loss 0.8959 Accuracy 0.7623\n","Epoch 2 Loss 0.8958 Accuracy 0.7623\n","Epoch 2 Loss 0.8957 Accuracy 0.7623\n","Epoch 2 Loss 0.8956 Accuracy 0.7623\n","Epoch 2 Loss 0.8954 Accuracy 0.7624\n","Epoch 2 Loss 0.8954 Accuracy 0.7624\n","Epoch 2 Loss 0.8952 Accuracy 0.7624\n","Epoch 2 Loss 0.8951 Accuracy 0.7624\n","Epoch 2 Loss 0.8950 Accuracy 0.7625\n","Epoch 2 Loss 0.8949 Accuracy 0.7625\n","Epoch 2 Loss 0.8949 Accuracy 0.7625\n","Epoch 2 Loss 0.8948 Accuracy 0.7625\n","Epoch 2 Loss 0.8947 Accuracy 0.7625\n","Epoch 2 Loss 0.8946 Accuracy 0.7625\n","Epoch 2 Loss 0.8945 Accuracy 0.7626\n","Epoch 2 Loss 0.8944 Accuracy 0.7626\n","Epoch 2 Loss 0.8943 Accuracy 0.7626\n","Epoch 2 Loss 0.8942 Accuracy 0.7626\n","Epoch 2 Loss 0.8941 Accuracy 0.7626\n","Epoch 2 Loss 0.8940 Accuracy 0.7627\n","Epoch 2 Loss 0.8939 Accuracy 0.7627\n","Epoch 2 Loss 0.8938 Accuracy 0.7627\n","Epoch 2 Loss 0.8936 Accuracy 0.7627\n","Epoch 2 Loss 0.8936 Accuracy 0.7627\n","Epoch 2 Loss 0.8934 Accuracy 0.7628\n","Epoch 2 Loss 0.8933 Accuracy 0.7628\n","Epoch 2 Loss 0.8931 Accuracy 0.7628\n","Epoch 2 Loss 0.8930 Accuracy 0.7629\n","Epoch 2 Loss 0.8929 Accuracy 0.7629\n","Epoch 2 Loss 0.8928 Accuracy 0.7629\n","Epoch 2 Loss 0.8927 Accuracy 0.7629\n","Epoch 2 Loss 0.8926 Accuracy 0.7629\n","Epoch 2 Loss 0.8925 Accuracy 0.7630\n","Epoch 2 Loss 0.8924 Accuracy 0.7630\n","Epoch 2 Loss 0.8922 Accuracy 0.7630\n","Epoch 2 Loss 0.8921 Accuracy 0.7630\n","Epoch 2 Loss 0.8920 Accuracy 0.7631\n","Epoch 2 Loss 0.8919 Accuracy 0.7631\n","Epoch 2 Loss 0.8917 Accuracy 0.7631\n","Epoch 2 Loss 0.8916 Accuracy 0.7631\n","Epoch 2 Loss 0.8915 Accuracy 0.7631\n","Epoch 2 Loss 0.8914 Accuracy 0.7632\n","Epoch 2 Loss 0.8912 Accuracy 0.7632\n","Epoch 2 Loss 0.8911 Accuracy 0.7632\n","Epoch 2 Loss 0.8911 Accuracy 0.7632\n","Epoch 2 Loss 0.8910 Accuracy 0.7633\n","Epoch 2 Loss 0.8909 Accuracy 0.7633\n","Epoch 2 Loss 0.8907 Accuracy 0.7633\n","Epoch 2 Loss 0.8907 Accuracy 0.7633\n","Epoch 2 Loss 0.8905 Accuracy 0.7633\n","Epoch 2 Loss 0.8905 Accuracy 0.7633\n","Epoch 2 Loss 0.8904 Accuracy 0.7634\n","Epoch 2 Loss 0.8903 Accuracy 0.7634\n","Epoch 2 Loss 0.8902 Accuracy 0.7634\n","Epoch 2 Loss 0.8901 Accuracy 0.7634\n","Epoch 2 Loss 0.8900 Accuracy 0.7634\n","Epoch 2 Loss 0.8898 Accuracy 0.7635\n","Epoch 2 Loss 0.8897 Accuracy 0.7635\n","Epoch 2 Loss 0.8896 Accuracy 0.7635\n","Epoch 2 Loss 0.8894 Accuracy 0.7635\n","Epoch 2 Loss 0.8893 Accuracy 0.7636\n","Epoch 2 Loss 0.8892 Accuracy 0.7636\n","Epoch 2 Loss 0.8891 Accuracy 0.7636\n","Epoch 2 Loss 0.8890 Accuracy 0.7636\n","Epoch 2 Loss 0.8889 Accuracy 0.7636\n","Epoch 2 Loss 0.8888 Accuracy 0.7637\n","Epoch 2 Loss 0.8887 Accuracy 0.7637\n","Epoch 2 Loss 0.8886 Accuracy 0.7637\n","Epoch 2 Loss 0.8885 Accuracy 0.7637\n","Epoch 2 Loss 0.8884 Accuracy 0.7637\n","Epoch 2 Loss 0.8883 Accuracy 0.7638\n","Epoch 2 Loss 0.8882 Accuracy 0.7638\n","Epoch 2 Loss 0.8881 Accuracy 0.7638\n","Epoch 2 Loss 0.8880 Accuracy 0.7638\n","Epoch 2 Loss 0.8879 Accuracy 0.7639\n","Epoch 2 Loss 0.8878 Accuracy 0.7639\n","Epoch 2 Loss 0.8876 Accuracy 0.7639\n","Epoch 2 Loss 0.8876 Accuracy 0.7639\n","Epoch 2 Loss 0.8874 Accuracy 0.7639\n","Epoch 2 Loss 0.8873 Accuracy 0.7640\n","Epoch 2 Loss 0.8873 Accuracy 0.7640\n","Epoch 2 Loss 0.8871 Accuracy 0.7640\n","Epoch 2 Loss 0.8870 Accuracy 0.7640\n","Epoch 2 Loss 0.8868 Accuracy 0.7641\n","Epoch 2 Loss 0.8867 Accuracy 0.7641\n","Epoch 2 Loss 0.8865 Accuracy 0.7641\n","Epoch 2 Loss 0.8864 Accuracy 0.7641\n","Epoch 2 Loss 0.8863 Accuracy 0.7641\n","Epoch 2 Loss 0.8863 Accuracy 0.7642\n","Epoch 2 Loss 0.8861 Accuracy 0.7642\n","Epoch 2 Loss 0.8861 Accuracy 0.7642\n","Epoch 2 Loss 0.8859 Accuracy 0.7642\n","Epoch 2 Loss 0.8858 Accuracy 0.7643\n","Epoch 2 Loss 0.8857 Accuracy 0.7643\n","Epoch 2 Loss 0.8856 Accuracy 0.7643\n","Epoch 2 Loss 0.8855 Accuracy 0.7643\n","Epoch 2 Loss 0.8854 Accuracy 0.7643\n","Epoch 2 Loss 0.8853 Accuracy 0.7643\n","Epoch 2 Loss 0.8852 Accuracy 0.7644\n","Epoch 2 Loss 0.8851 Accuracy 0.7644\n","Epoch 2 Loss 0.8850 Accuracy 0.7644\n","Epoch 2 Loss 0.8848 Accuracy 0.7644\n","Epoch 2 Loss 0.8847 Accuracy 0.7645\n","Epoch 2 Loss 0.8846 Accuracy 0.7645\n","Epoch 2 Loss 0.8845 Accuracy 0.7645\n","Epoch 2 Loss 0.8844 Accuracy 0.7645\n","Epoch 2 Loss 0.8843 Accuracy 0.7645\n","Epoch 2 Loss 0.8842 Accuracy 0.7646\n","Epoch 2 Loss 0.8841 Accuracy 0.7646\n","Epoch 2 Loss 0.8840 Accuracy 0.7646\n","Epoch 2 Loss 0.8839 Accuracy 0.7646\n","Epoch 2 Loss 0.8838 Accuracy 0.7646\n","Epoch 2 Loss 0.8837 Accuracy 0.7647\n","Epoch 2 Loss 0.8836 Accuracy 0.7647\n","Epoch 2 Loss 0.8835 Accuracy 0.7647\n","Epoch 2 Loss 0.8834 Accuracy 0.7647\n","Epoch 2 Loss 0.8833 Accuracy 0.7647\n","Epoch 2 Loss 0.8832 Accuracy 0.7648\n","Epoch 2 Loss 0.8831 Accuracy 0.7648\n","Epoch 2 Loss 0.8830 Accuracy 0.7648\n","Epoch 2 Loss 0.8829 Accuracy 0.7648\n","Epoch 2 Loss 0.8828 Accuracy 0.7649\n","Epoch 2 Loss 0.8827 Accuracy 0.7649\n","Epoch 2 Loss 0.8826 Accuracy 0.7649\n","Epoch 2 Loss 0.8825 Accuracy 0.7649\n","Epoch 2 Loss 0.8824 Accuracy 0.7649\n","Epoch 2 Loss 0.8823 Accuracy 0.7649\n","Epoch 2 Loss 0.8822 Accuracy 0.7650\n","Epoch 2 Loss 0.8821 Accuracy 0.7650\n","Epoch 2 Loss 0.8821 Accuracy 0.7650\n","Epoch 2 Loss 0.8820 Accuracy 0.7650\n","Epoch 2 Loss 0.8819 Accuracy 0.7650\n","Epoch 2 Loss 0.8818 Accuracy 0.7650\n","Epoch 2 Loss 0.8817 Accuracy 0.7651\n","Epoch 2 Loss 0.8816 Accuracy 0.7651\n","Epoch 2 Loss 0.8815 Accuracy 0.7651\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"R7_hx3wdKaIx"},"source":["## Testing"]},{"cell_type":"code","metadata":{"id":"ArqWgY5xKhRa"},"source":["def evaluate(inp_sentence):\n","  inp_sentence = [VOCAB_SIZE_ES-2]+tokenizer_es.encode(inp_sentence)+ [VOCAB_SIZE_ES-1]\n","  enc_input = tf.expand_dims(inp_sentence, axis=0)\n","  output = tf.expand_dims([VOCAB_SIZE_ES-2], axis=0)\n","  for _ in range(MAX_LENGTH):\n","    predictions = transformer(enc_input, output, False)\n","    predictions = predictions[:,-1:,:]\n","    predicted_id = tf.cast(tf.argmax(predictions,axis=-1), tf.int32)\n","    if predicted_id == VOCAB_SIZE_ES -1:\n","      return tf.squeeze(output,axis=0)\n","    output = tf.concat([output, predicted_id], axis=-1)\n","  return tf.squeeze(output,axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qq-72oQ13pLK"},"source":["def sort_sentences(sentence):\n","  output = evaluate(sentence).numpy()\n","  predicted_sentence = tokenizer_es.decode([id for id in output if id<(VOCAB_SIZE_ES-2)])\n","  return predicted_sentence"],"execution_count":null,"outputs":[]}]}